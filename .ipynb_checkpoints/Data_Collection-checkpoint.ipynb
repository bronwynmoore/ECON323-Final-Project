{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6baf244e-b44c-4b03-971d-095f972dabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a4168-7eb1-40a2-84c5-bc38488f3ae4",
   "metadata": {},
   "source": [
    "# ECON 323 FINAL PROJECT: WEB-SCRAPING CODE\n",
    "\n",
    "In this workbook I use web-scraping techniques to create the data set used for my analysis. I am scraping from Amazon's e-book featured listings to get data on books that appear in the first 30 pages of the [Kindle Store](https://www.amazon.ca/s?rh=n%3A2980423011&fs=true&ref=lp_2980423011_sar). The code is loosely inspired by [this tutorial](https://www.datacamp.com/community/tutorials/amazon-web-scraping-using-beautifulsoup). I use the Beautiful Soup package to navigate the html I extracted from Amazon.\n",
    "\n",
    "### 1. HTML Extraction\n",
    "\n",
    "The first step is to write a function which will request the needed html for all the listings in a specific page range. The attributes I am interested in scraping are not all available in the information shown on the listings page so for each book on the page I need to request the html for each books individual page. We want to do this for more then one page of listings, as there are only around 16 non-sponsored listings per page. \n",
    "\n",
    "In the images below, for each of the books listed on the page on the right we want to extract the html for the page on the left.\n",
    "\n",
    "![alt text](listings.png \"Title\")\n",
    "\n",
    "For each page in the rage, the get soup function performs the following major steps: \n",
    "- First it requests the html for the listings page and applies the BeautifulSoup function to create a navigable object.\n",
    "- Then it finds all of the non-sponsored listings on the page.\n",
    "- Then, iterating through each lisitng, it finds the link to the individual page for the book and requests the html for that page.\n",
    "- Finally, it applies the BeautifulSoup function to the individual book page and adds it to a list.\n",
    "\n",
    "The function returns a list of soup objects for all the books listed.\n",
    "\n",
    "Unsurprisingly, Amazon has some bot-detection so we need to make our requests look realistic, in order to do this we need to send some headers with our requests. If you submit too many requests in a row from the same User Agent, you encounter a recapcha. To work around this I set up a list of possible user agent, and for each page the function randomly selects a new user agents from the list to use in the header. Nevertheless, I still encounter issues relatively frequently (~ 1 in 10 requests fail to pull any data) but this is a huge improvement from where I started. From what I can find, there are a number of other steps you can take to reduce chances of running into issues with bot detection but they are beyond my ability to implement within the time frame. As it is, we are not reliably getting data on all the books in the requested page range, but this is not of material consequence for the issue at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee5ece7-ef0a-452a-9c85-eb94a4a62a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set possible user agents\n",
    "user_agent_list = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299\",\n",
    "                   \"Mozilla/5.0 (Windows NT 5.1; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
    "                   \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\",\n",
    "                   \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "                   \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\",\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7\",\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\",\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/601.5.17 (KHTML, like Gecko) Version/9.1 Safari/601.5.17\",\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/600.6.3 (KHTML, like Gecko) Version/8.0.6 Safari/600.6.3\",\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Safari/602.1.50\",\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/600.4.10 (KHTML, like Gecko) Version/8.0.4 Safari/600.4.10\"\n",
    "                   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\"\n",
    "                  ]\n",
    "\n",
    "def get_soup(page_start,page_end):\n",
    "    \"\"\"\n",
    "    Takes a range of pages and returns a list of the html from the individual listings of all books \n",
    "    listed on the pages indicated.\n",
    "    \"\"\"\n",
    "    book_soups = []\n",
    "    for pageNo in range(page_start , page_end+1):\n",
    "        #randomly select a user agent\n",
    "        user_agent = random.choice(user_agent_list)\n",
    "        #set header\n",
    "        headers = {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "                   \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "                   \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                   \"User-Agent\": user_agent}\n",
    "        \n",
    "        #request html for listings page\n",
    "        r = requests.get('https://www.amazon.ca/s?i=digital-text&rh=n%3A2980423011&fs=true&page='+str(pageNo)+'&qid=1640233972&ref=sr_pg_'+str(pageNo), headers=headers)\n",
    "        content = r.content\n",
    "        soup = BeautifulSoup(content)\n",
    "        \n",
    "        #itterate over individual listings\n",
    "        for d in soup.findAll('div', class_ = \"s-result-item s-asin sg-col-0-of-12 sg-col-16-of-20 sg-col s-widget-spacing-small sg-col-12-of-16\"):\n",
    "            #find link to individual listing\n",
    "            links = d.find('a', class_ = \"a-link-normal s-no-outline\")\n",
    "            link = links['href']\n",
    "            #request html for individual listing\n",
    "            book_r = requests.get('https://www.amazon.ca'+link, headers=headers)\n",
    "            book_content = book_r.content\n",
    "            book_soup = BeautifulSoup(book_content)\n",
    "            \n",
    "            book_soups.append(book_soup)\n",
    "            \n",
    "    return(book_soups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0a0da-231d-4523-9c96-fde5c466a83b",
   "metadata": {},
   "source": [
    "### 2. Scraping Data\n",
    "\n",
    "Next I define a function that will extract all the wanted attributes from the html object acquired for each books. I am interested in the following information:\n",
    "- Title\n",
    "- Author\n",
    "- Price\n",
    "- Inclusion in Kindle Unlimited\n",
    "- Ranking\n",
    "- Rating\n",
    "- Number of Ratings\n",
    "- Blurb\n",
    "- Reviews (title, rating, text content)\n",
    "\n",
    "![alt text](attribute.png \"Title\")\n",
    "\n",
    "For each of these attributes, the get data function uses the infrastructure of the BeautifulSoup package to navigate the html of the web-page to find the given attribute and add it to a list. If the attribute can not be found then NA is appended instead. The function returns a list of all the attributes. \n",
    "\n",
    "For the reviews, I extract only the reviews shown in the \"top reviews\" section. There is a varying number of reviews that show up in this section for each listing. The review related attributes are stored as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342b9074-544d-4520-84cd-d484d322eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(s):\n",
    "    data_list = []\n",
    "    \n",
    "    #book title\n",
    "    title = s.find('span', id = 'productTitle')\n",
    "    data_list.append(title.text if title is not None else np.nan)\n",
    "    \n",
    "    #author (allows for more than one)\n",
    "    author_list = s.findAll('span', class_ = 'author notFaded')\n",
    "    authors = []\n",
    "    for a in author_list:\n",
    "        author = a.find('a', class_ = 'a-link-normal contributorNameID')\n",
    "        if author is not None:\n",
    "            authors.append(author.text)\n",
    "    data_list.append(authors if authors is not [] else np.nan)\n",
    "    \n",
    "    #price\n",
    "    price = s.find('span', id = 'kindle-price')\n",
    "    data_list.append(price.text if price is not None else np.nan)\n",
    "    \n",
    "    #kindle unlimited \n",
    "    ku_icon = s.find('i', class_ = 'a-icon a-icon-kindle-unlimited a-icon-medium')\n",
    "    data_list.append(1 if ku_icon is not None else 0)\n",
    "    \n",
    "    #ranking\n",
    "    product_details = s.find('div', id=\"detailBullets_feature_div\")\n",
    "    if product_details is not None:\n",
    "        detail_groups = product_details.findAll('ul', class_ = \"a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list\")\n",
    "        ranks = detail_groups[1].find('span', class_ = \"a-list-item\")\n",
    "        data_list.append(ranks.text if ranks is not None else np.nan)\n",
    "    \n",
    "    #overall rating\n",
    "        rating = detail_groups[2].find('span', class_ = 'a-icon-alt')\n",
    "        data_list.append(rating.text if rating is not None else np.nan)\n",
    "    \n",
    "    #number of ratings\n",
    "    rating_count = s.find('span', attrs={'id':'acrCustomerReviewText', 'class':'a-size-base'})\n",
    "    data_list.append(rating_count.text if rating_count is not None else np.nan)\n",
    "    \n",
    "    #blurb\n",
    "    blurb = s.find('div', id = 'bookDescription_feature_div')\n",
    "    data_list.append(blurb.text if blurb is not None else np.nan)\n",
    "    \n",
    "    #reviews\n",
    "    reviews = s.find('div', class_=\"a-section a-spacing-large reviews-content filterable-reviews-content celwidget\")\n",
    "    \n",
    "    reviews_ratings = []\n",
    "    reviews_titles = []\n",
    "    reviews_texts = []\n",
    "    \n",
    "    if reviews is not None:\n",
    "        reviews_list = reviews.findAll('div', class_=\"a-section review aok-relative\")\n",
    "    \n",
    "        for r in reviews_list:\n",
    "            #get rating from all top reviews\n",
    "            review_rating = r.find('a', class_ = \"a-link-normal\")\n",
    "            reviews_ratings.append(review_rating['title'] if review_rating is not None else np.nan)\n",
    "        \n",
    "            #get titles from all top reviews\n",
    "            review_title = r.find('a', class_ = \"a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold\")\n",
    "            reviews_titles.append(review_title.text if review_title is not None else np.nan)\n",
    "        \n",
    "            #get text from all top reviews\n",
    "            review_text = r.find('div', class_ = 'a-expander-content reviewText review-text-content a-expander-partial-collapse-content')\n",
    "            reviews_texts.append(review_text.text if blurb is not None else np.nan)\n",
    "    \n",
    "    data_list.append(reviews_ratings if reviews_ratings is not [] else np.nan)\n",
    "    data_list.append(reviews_titles if reviews_titles is not [] else np.nan)\n",
    "    data_list.append(reviews_texts if reviews_texts is not [] else np.nan)\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a8db1-920b-4462-a143-22d762e6bbd5",
   "metadata": {},
   "source": [
    "### 3. Apply Functions\n",
    "\n",
    "Next we apply our two functions, I do this in three separate 10 page intervals because I was running into issues with the kernel restarting randomly in the middle of the code executing and losing all the progress. \n",
    "\n",
    "For each group of ten pages, I first apply the get_soup function I defined previously, to extract the html for all the individual listings. Then I use a for loop to apply the get data function to each listing. Finally I convert the data to a data frame and export it as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0555aa-a6db-497b-9426-9bd084da8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_soups_1_10 = get_soup(1,10)\n",
    "\n",
    "data_1_10 = []\n",
    "for b in book_soups_1_10:\n",
    "    b_data = get_data(b)\n",
    "    data_1_10.append(b_data)\n",
    "    \n",
    "book_data_1_10 = pd.DataFrame(data_1_10,columns=['Book Name','Author', 'Price', 'KU', 'Rankings','Rating','Customers_Rated','Blurb', 'Reviews_Ratings', 'Reviews_Titles', 'Reviews_Texts' ])\n",
    "book_data_1_10.to_csv('amazon_books_1_10.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e209f4d1-2889-4c67-b6e3-2b708aa89817",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_soups_11_20 = get_soup(11,20)\n",
    "\n",
    "data_11_20 = []\n",
    "for b in book_soups_11_20:\n",
    "    b_data = get_data(b)\n",
    "    data_11_20.append(b_data)\n",
    "    \n",
    "book_data_11_20 = pd.DataFrame(data_11_20,columns=['Book Name','Author', 'Price', 'KU', 'Rankings','Rating','Customers_Rated','Blurb', 'Reviews_Ratings', 'Reviews_Titles', 'Reviews_Texts' ])\n",
    "book_data_11_20.to_csv('amazon_books_11_20.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661cbff5-83de-4185-bf4e-9af96678df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_soups_21_30 = get_soup(21,30)\n",
    "\n",
    "data_21_30 = []\n",
    "for b in book_soups_21_30:\n",
    "    b_data = get_data(b)\n",
    "    data_21_30.append(b_data)\n",
    "    \n",
    "book_data_21_30 = pd.DataFrame(data_21_30,columns=['Book Name','Author', 'Price', 'KU', 'Rankings','Rating','Customers_Rated','Blurb', 'Reviews_Ratings', 'Reviews_Titles', 'Reviews_Texts' ])\n",
    "book_data_21_30.to_csv('amazon_books_21_30.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184a4ac2-1a94-4efc-b4ed-b6a748a8370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_soups_31_40 = get_soup(31,40)\n",
    "\n",
    "data_31_40 = []\n",
    "for b in book_soups_31_40:\n",
    "    b_data = get_data(b)\n",
    "    data_31_40.append(b_data)\n",
    "    \n",
    "book_data_31_40 = pd.DataFrame(data_31_40,columns=['Book Name','Author', 'Price', 'KU', 'Rankings','Rating','Customers_Rated','Blurb', 'Reviews_Ratings', 'Reviews_Titles', 'Reviews_Texts' ])\n",
    "book_data_31_40.to_csv('amazon_books_31_40.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c290b16f-c330-4bd3-b463-0a3a8c04b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_soups_41_50 = get_soup(41,50)\n",
    "\n",
    "data_41_50 = []\n",
    "for b in book_soups_41_50:\n",
    "    b_data = get_data(b)\n",
    "    data_41_50.append(b_data)\n",
    "    \n",
    "book_data_41_50 = pd.DataFrame(data_41_50,columns=['Book Name','Author', 'Price', 'KU', 'Rankings','Rating','Customers_Rated','Blurb', 'Reviews_Ratings', 'Reviews_Titles', 'Reviews_Texts' ])\n",
    "book_data_41_50.to_csv('amazon_books_41_50.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81288d2f-8fa7-4a2c-b31a-a582ba55182f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
